{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../EEG_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, :4]\n",
    "Y = data.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32829469, -0.1665177 ,  0.56468612],\n",
       "       [-0.85307294, -0.80708277, -0.10424572],\n",
       "       [-0.0170797 , -0.7498735 ,  0.09149241],\n",
       "       ...,\n",
       "       [-0.2469368 , -0.5131385 ,  0.25170445],\n",
       "       [-0.09700678, -0.63461101, -0.01072684],\n",
       "       [ 0.36853725, -0.3391129 ,  0.342383  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = []\n",
    "for x in range(0, len(X[\"eeg1\"])):\n",
    "    X_train.append([X[\"eeg1\"][x], X[\"eeg3\"][x], X[\"eeg4\"][x]])\n",
    "X_train = np.array(X_train, dtype='float')\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = []\n",
    "for y in range(0, len(Y[\"focused\"])):\n",
    "    Y_train.append([Y[\"focused\"][y]])\n",
    "Y_train = np.array(Y_train, dtype='float')\n",
    "\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG_Model(\n",
      "  (fc1): Linear(in_features=3, out_features=15, bias=True)\n",
      "  (fc2): Linear(in_features=15, out_features=15, bias=True)\n",
      "  (fc3): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class EEG_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEG_Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 15)\n",
    "        self.fc2 = nn.Linear(15, 15)\n",
    "        self.fc3 = nn.Linear(15, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc3(x))        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = EEG_Model()\n",
    "model = model.double()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.149250\n",
      "Epoch: 2 \tTraining Loss: 0.141385\n",
      "Epoch: 3 \tTraining Loss: 0.138883\n",
      "Epoch: 4 \tTraining Loss: 0.135969\n",
      "Epoch: 5 \tTraining Loss: 0.135684\n",
      "Epoch: 6 \tTraining Loss: 0.134967\n",
      "Epoch: 7 \tTraining Loss: 0.135143\n",
      "Epoch: 8 \tTraining Loss: 0.134957\n",
      "Epoch: 9 \tTraining Loss: 0.134226\n",
      "Epoch: 10 \tTraining Loss: 0.133545\n",
      "Epoch: 11 \tTraining Loss: 0.134355\n",
      "Epoch: 12 \tTraining Loss: 0.133614\n",
      "Epoch: 13 \tTraining Loss: 0.134129\n",
      "Epoch: 14 \tTraining Loss: 0.132967\n",
      "Epoch: 15 \tTraining Loss: 0.133210\n",
      "Epoch: 16 \tTraining Loss: 0.133285\n",
      "Epoch: 17 \tTraining Loss: 0.133083\n",
      "Epoch: 18 \tTraining Loss: 0.133789\n",
      "Epoch: 19 \tTraining Loss: 0.132950\n",
      "Epoch: 20 \tTraining Loss: 0.132553\n",
      "Epoch: 21 \tTraining Loss: 0.134432\n",
      "Epoch: 22 \tTraining Loss: 0.133546\n",
      "Epoch: 23 \tTraining Loss: 0.133634\n",
      "Epoch: 24 \tTraining Loss: 0.132976\n",
      "Epoch: 25 \tTraining Loss: 0.133070\n",
      "Epoch: 26 \tTraining Loss: 0.132522\n",
      "Epoch: 27 \tTraining Loss: 0.133617\n",
      "Epoch: 28 \tTraining Loss: 0.133105\n",
      "Epoch: 29 \tTraining Loss: 0.132564\n",
      "Epoch: 30 \tTraining Loss: 0.132437\n",
      "Epoch: 31 \tTraining Loss: 0.131885\n",
      "Epoch: 32 \tTraining Loss: 0.132569\n",
      "Epoch: 33 \tTraining Loss: 0.132683\n",
      "Epoch: 34 \tTraining Loss: 0.132325\n",
      "Epoch: 35 \tTraining Loss: 0.132654\n",
      "Epoch: 36 \tTraining Loss: 0.132505\n",
      "Epoch: 37 \tTraining Loss: 0.131871\n",
      "Epoch: 38 \tTraining Loss: 0.132716\n",
      "Epoch: 39 \tTraining Loss: 0.132140\n",
      "Epoch: 40 \tTraining Loss: 0.131467\n",
      "Epoch: 41 \tTraining Loss: 0.132011\n",
      "Epoch: 42 \tTraining Loss: 0.132266\n",
      "Epoch: 43 \tTraining Loss: 0.132067\n",
      "Epoch: 44 \tTraining Loss: 0.132878\n",
      "Epoch: 45 \tTraining Loss: 0.131699\n",
      "Epoch: 46 \tTraining Loss: 0.131925\n",
      "Epoch: 47 \tTraining Loss: 0.131954\n",
      "Epoch: 48 \tTraining Loss: 0.132374\n",
      "Epoch: 49 \tTraining Loss: 0.131973\n",
      "Epoch: 50 \tTraining Loss: 0.131973\n",
      "Epoch: 51 \tTraining Loss: 0.132557\n",
      "Epoch: 52 \tTraining Loss: 0.132812\n",
      "Epoch: 53 \tTraining Loss: 0.131876\n",
      "Epoch: 54 \tTraining Loss: 0.131750\n",
      "Epoch: 55 \tTraining Loss: 0.132507\n",
      "Epoch: 56 \tTraining Loss: 0.131626\n",
      "Epoch: 57 \tTraining Loss: 0.131501\n",
      "Epoch: 58 \tTraining Loss: 0.130889\n",
      "Epoch: 59 \tTraining Loss: 0.131735\n",
      "Epoch: 60 \tTraining Loss: 0.131644\n",
      "Epoch: 61 \tTraining Loss: 0.131689\n",
      "Epoch: 62 \tTraining Loss: 0.130850\n",
      "Epoch: 63 \tTraining Loss: 0.131620\n",
      "Epoch: 64 \tTraining Loss: 0.131337\n",
      "Epoch: 65 \tTraining Loss: 0.132280\n",
      "Epoch: 66 \tTraining Loss: 0.131065\n",
      "Epoch: 67 \tTraining Loss: 0.131444\n",
      "Epoch: 68 \tTraining Loss: 0.130904\n",
      "Epoch: 69 \tTraining Loss: 0.131456\n",
      "Epoch: 70 \tTraining Loss: 0.131619\n",
      "Epoch: 71 \tTraining Loss: 0.129905\n",
      "Epoch: 72 \tTraining Loss: 0.130896\n",
      "Epoch: 73 \tTraining Loss: 0.131256\n",
      "Epoch: 74 \tTraining Loss: 0.130900\n",
      "Epoch: 75 \tTraining Loss: 0.131081\n",
      "Epoch: 76 \tTraining Loss: 0.131211\n",
      "Epoch: 77 \tTraining Loss: 0.130720\n",
      "Epoch: 78 \tTraining Loss: 0.131354\n",
      "Epoch: 79 \tTraining Loss: 0.131559\n",
      "Epoch: 80 \tTraining Loss: 0.131427\n",
      "Epoch: 81 \tTraining Loss: 0.131384\n",
      "Epoch: 82 \tTraining Loss: 0.130382\n",
      "Epoch: 83 \tTraining Loss: 0.130128\n",
      "Epoch: 84 \tTraining Loss: 0.131227\n",
      "Epoch: 85 \tTraining Loss: 0.130745\n",
      "Epoch: 86 \tTraining Loss: 0.129772\n",
      "Epoch: 87 \tTraining Loss: 0.130447\n",
      "Epoch: 88 \tTraining Loss: 0.131077\n",
      "Epoch: 89 \tTraining Loss: 0.131376\n",
      "Epoch: 90 \tTraining Loss: 0.130827\n",
      "Epoch: 91 \tTraining Loss: 0.130870\n",
      "Epoch: 92 \tTraining Loss: 0.130414\n",
      "Epoch: 93 \tTraining Loss: 0.131191\n",
      "Epoch: 94 \tTraining Loss: 0.130539\n",
      "Epoch: 95 \tTraining Loss: 0.130573\n",
      "Epoch: 96 \tTraining Loss: 0.130565\n",
      "Epoch: 97 \tTraining Loss: 0.130940\n",
      "Epoch: 98 \tTraining Loss: 0.130889\n",
      "Epoch: 99 \tTraining Loss: 0.129974\n",
      "Epoch: 100 \tTraining Loss: 0.130840\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for i in range(len(X_train)):\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        x = X_train[i]\n",
    "        x = torch.from_numpy(x)\n",
    "        \n",
    "        y = Y_train[i]\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update running training loss\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(X_train)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EEG_Model' object has no attribute 'save_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-466584a41224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EEG-Model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    589\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 591\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EEG_Model' object has no attribute 'save_state_dict'"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'EEG_Model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
