{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../EEG_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, :4]\n",
    "Y = data.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08119012, -1.06034029, -0.05326613],\n",
       "       [-0.09418102, -1.05526507, -0.07929981],\n",
       "       [-0.10742053, -1.03184092, -0.08800521],\n",
       "       ...,\n",
       "       [ 0.03256479, -0.6228168 ,  0.07464068],\n",
       "       [ 0.02238401, -0.62      ,  0.06881502],\n",
       "       [-0.0067838 , -0.6125872 ,  0.05700178]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = []\n",
    "for x in range(0, len(X[\"eeg1\"])):\n",
    "    X_train.append([X[\"eeg1\"][x], X[\"eeg3\"][x], X[\"eeg4\"][x]])\n",
    "X_train = np.array(X_train, dtype='float')\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = []\n",
    "for y in range(0, len(Y[\"focused\"])):\n",
    "    Y_train.append([Y[\"focused\"][y]])\n",
    "Y_train = np.array(Y_train, dtype='float')\n",
    "\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG_Model(\n",
      "  (fc1): Linear(in_features=3, out_features=15, bias=True)\n",
      "  (fc2): Linear(in_features=15, out_features=15, bias=True)\n",
      "  (fc3): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class EEG_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEG_Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 15)\n",
    "        self.fc2 = nn.Linear(15, 15)\n",
    "        self.fc3 = nn.Linear(15, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.sigmoid(self.fc3(x))        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = EEG_Model()\n",
    "model = model.double()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.012405\n",
      "Epoch: 2 \tTraining Loss: 0.009148\n",
      "Epoch: 3 \tTraining Loss: 0.009281\n",
      "Epoch: 4 \tTraining Loss: 0.007980\n",
      "Epoch: 5 \tTraining Loss: 0.006587\n",
      "Epoch: 6 \tTraining Loss: 0.007991\n",
      "Epoch: 7 \tTraining Loss: 0.009688\n",
      "Epoch: 8 \tTraining Loss: 0.008765\n",
      "Epoch: 9 \tTraining Loss: 0.008064\n",
      "Epoch: 10 \tTraining Loss: 0.007687\n",
      "Epoch: 11 \tTraining Loss: 0.007969\n",
      "Epoch: 12 \tTraining Loss: 0.007026\n",
      "Epoch: 13 \tTraining Loss: 0.008577\n",
      "Epoch: 14 \tTraining Loss: 0.007444\n",
      "Epoch: 15 \tTraining Loss: 0.006018\n",
      "Epoch: 16 \tTraining Loss: 0.006144\n",
      "Epoch: 17 \tTraining Loss: 0.006154\n",
      "Epoch: 18 \tTraining Loss: 0.006612\n",
      "Epoch: 19 \tTraining Loss: 0.006403\n",
      "Epoch: 20 \tTraining Loss: 0.007061\n",
      "Epoch: 21 \tTraining Loss: 0.006612\n",
      "Epoch: 22 \tTraining Loss: 0.006924\n",
      "Epoch: 23 \tTraining Loss: 0.007125\n",
      "Epoch: 24 \tTraining Loss: 0.006600\n",
      "Epoch: 25 \tTraining Loss: 0.006924\n",
      "Epoch: 26 \tTraining Loss: 0.007003\n",
      "Epoch: 27 \tTraining Loss: 0.006584\n",
      "Epoch: 28 \tTraining Loss: 0.007054\n",
      "Epoch: 29 \tTraining Loss: 0.007900\n",
      "Epoch: 30 \tTraining Loss: 0.005833\n",
      "Epoch: 31 \tTraining Loss: 0.004971\n",
      "Epoch: 32 \tTraining Loss: 0.009529\n",
      "Epoch: 33 \tTraining Loss: 0.005173\n",
      "Epoch: 34 \tTraining Loss: 0.008352\n",
      "Epoch: 35 \tTraining Loss: 0.006458\n",
      "Epoch: 36 \tTraining Loss: 0.006004\n",
      "Epoch: 37 \tTraining Loss: 0.006936\n",
      "Epoch: 38 \tTraining Loss: 0.005789\n",
      "Epoch: 39 \tTraining Loss: 0.007458\n",
      "Epoch: 40 \tTraining Loss: 0.008174\n",
      "Epoch: 41 \tTraining Loss: 0.008889\n",
      "Epoch: 42 \tTraining Loss: 0.014115\n",
      "Epoch: 43 \tTraining Loss: 0.006146\n",
      "Epoch: 44 \tTraining Loss: 0.009026\n",
      "Epoch: 45 \tTraining Loss: 0.007423\n",
      "Epoch: 46 \tTraining Loss: 0.006179\n",
      "Epoch: 47 \tTraining Loss: 0.006538\n",
      "Epoch: 48 \tTraining Loss: 0.006058\n",
      "Epoch: 49 \tTraining Loss: 0.009068\n",
      "Epoch: 50 \tTraining Loss: 0.007875\n",
      "Epoch: 51 \tTraining Loss: 0.024888\n",
      "Epoch: 52 \tTraining Loss: 0.011465\n",
      "Epoch: 53 \tTraining Loss: 0.010094\n",
      "Epoch: 54 \tTraining Loss: 0.010829\n",
      "Epoch: 55 \tTraining Loss: 0.009239\n",
      "Epoch: 56 \tTraining Loss: 0.007804\n",
      "Epoch: 57 \tTraining Loss: 0.008174\n",
      "Epoch: 58 \tTraining Loss: 0.007942\n",
      "Epoch: 59 \tTraining Loss: 0.009301\n",
      "Epoch: 60 \tTraining Loss: 0.008556\n",
      "Epoch: 61 \tTraining Loss: 0.011852\n",
      "Epoch: 62 \tTraining Loss: 0.010214\n",
      "Epoch: 63 \tTraining Loss: 0.013995\n",
      "Epoch: 64 \tTraining Loss: 0.012007\n",
      "Epoch: 65 \tTraining Loss: 0.011978\n",
      "Epoch: 66 \tTraining Loss: 0.014491\n",
      "Epoch: 67 \tTraining Loss: 0.014120\n",
      "Epoch: 68 \tTraining Loss: 0.017741\n",
      "Epoch: 69 \tTraining Loss: 0.012548\n",
      "Epoch: 70 \tTraining Loss: 0.010313\n",
      "Epoch: 71 \tTraining Loss: 0.011404\n",
      "Epoch: 72 \tTraining Loss: 0.018151\n",
      "Epoch: 73 \tTraining Loss: 0.015752\n",
      "Epoch: 74 \tTraining Loss: 0.009424\n",
      "Epoch: 75 \tTraining Loss: 0.010529\n",
      "Epoch: 76 \tTraining Loss: 0.011167\n",
      "Epoch: 77 \tTraining Loss: 0.019414\n",
      "Epoch: 78 \tTraining Loss: 0.024165\n",
      "Epoch: 79 \tTraining Loss: 0.014959\n",
      "Epoch: 80 \tTraining Loss: 0.009611\n",
      "Epoch: 81 \tTraining Loss: 0.010364\n",
      "Epoch: 82 \tTraining Loss: 0.010343\n",
      "Epoch: 83 \tTraining Loss: 0.010445\n",
      "Epoch: 84 \tTraining Loss: 0.012793\n",
      "Epoch: 85 \tTraining Loss: 0.011566\n",
      "Epoch: 86 \tTraining Loss: 0.010089\n",
      "Epoch: 87 \tTraining Loss: 0.015911\n",
      "Epoch: 88 \tTraining Loss: 0.009878\n",
      "Epoch: 89 \tTraining Loss: 0.014348\n",
      "Epoch: 90 \tTraining Loss: 0.010887\n",
      "Epoch: 91 \tTraining Loss: 0.010147\n",
      "Epoch: 92 \tTraining Loss: 0.007715\n",
      "Epoch: 93 \tTraining Loss: 0.012608\n",
      "Epoch: 94 \tTraining Loss: 0.010775\n",
      "Epoch: 95 \tTraining Loss: 0.010311\n",
      "Epoch: 96 \tTraining Loss: 0.011455\n",
      "Epoch: 97 \tTraining Loss: 0.010497\n",
      "Epoch: 98 \tTraining Loss: 0.008579\n",
      "Epoch: 99 \tTraining Loss: 0.007237\n",
      "Epoch: 100 \tTraining Loss: 0.009354\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for i in range(len(X_train)):\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        x = X_train[i]\n",
    "        x = torch.from_numpy(x)\n",
    "        \n",
    "        y = Y_train[i]\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update running training loss\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(X_train)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'EEG_Model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
